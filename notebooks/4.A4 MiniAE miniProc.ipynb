{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Proc Model with MiniAE to demonstrate the use of LTN in an Autoencoder\n",
    "1. Generate the data\n",
    "2. Train the Autoencoder\n",
    "3. Modify the AE with LTN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Proc Model\n",
    "### Process\n",
    "\n",
    "e1 -> e2 -> e3 -> e4 -> e5\n",
    "\n",
    "event attributes = name, user\n",
    "\n",
    "event_names = [\"Create SC\", \"Approve SC\", \"Create PO\", \"Approve PO\", \"Pay\"]\n",
    "\n",
    "user_names = [\"Dev\", \"Chantal\", \"Seokju\", \"Jonas\", \"Kaly\"]\n",
    "\n",
    "### Valid traces\n",
    "1. [\"Create SC\", \"Approve SC\", \"Create PO\", \"Approve PO\", \"Pay\"]\n",
    "1. [\"Create SC\", \"Create PO\", \"Approve SC\", \"Approve PO\", \"Pay\"]\n",
    "\n",
    "### Event User Mapping\n",
    "1. \"Create SC\" : \"Dev\", \"Chantal\" \n",
    "2. \"Approve SC\" : \"Kaly\"\n",
    "3. \"Create PO\" : \"Dev\", \"Jonas\"\n",
    "4. \"Approve PO\" : \"Kaly\"\n",
    "5. \"Pay\" : \"Seokju\"\n",
    "\n",
    "### Data\n",
    "1. Traces = 1000\n",
    "2. p_anomaly = 0.3 # This means that the possibility that a given trace is anomalous is 0.3\n",
    "3. Anomaly types:\n",
    "    1. Control flow: irregular flow ordering\n",
    "    2. Attribute: Wrong attributes assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU found\n",
      "Memory growth set\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import ltn\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU found\")\n",
    "    print(\"Memory growth set\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event(dict):\n",
    "    \"\"\"\n",
    "    Event class with following keys:\n",
    "    :name: name of the event\n",
    "    :user: user who performed the event\n",
    "    :case_id: case id of the event\n",
    "    \"\"\"\n",
    "    pass\n",
    "class Case(list):\n",
    "    \"\"\"\n",
    "    Case is a list of traces\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders[name]:\n",
      "[('Approve PO', '0'), ('Approve SC', '1'), ('Create PO', '2'), ('Create SC', '3'), ('Pay', '4')]\n",
      "encoders[user]:\n",
      "[('Chantal', '0'), ('Dev', '1'), ('Jonas', '2'), ('Kaly', '3'), ('Seokju', '4')]\n",
      "Cases Distribution: [700, 148, 150]\n",
      "one_hot_flat shape: (998, 50)\n",
      "Raw features test:\n",
      "         name    user  case_id\n",
      "0   Create SC     Dev        0\n",
      "1  Approve SC    Kaly        0\n",
      "2   Create PO   Jonas        0\n",
      "3  Approve PO    Kaly        0\n",
      "4         Pay  Seokju        0\n",
      "Encoded Features test:\n",
      "[3 1 1 3 2 2 0 3 4 4]\n",
      "De encoding test:\n",
      "[[3 1 1 3 2 2 0 3 4 4]]\n"
     ]
    }
   ],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, max_cases=None, anomaly_probabilty=None):\n",
    "        self.event_names = set([\"Create SC\", \"Approve SC\", \"Create PO\", \"Approve PO\", \"Pay\"])\n",
    "        self.user_names = set([\"Dev\", \"Chantal\", \"Seokju\", \"Jonas\", \"Kaly\"])\n",
    "        self.valid_traces = [\n",
    "            [\"Create SC\", \"Approve SC\", \"Create PO\", \"Approve PO\", \"Pay\"],\n",
    "        ]\n",
    "        self.invalid_traces_control_flow = [\n",
    "            [\"Approve SC\", \"Create SC\", \"Create PO\", \"Approve PO\", \"Pay\"],\n",
    "            [\"Create SC\", \"Approve SC\", \"Approve PO\", \"Create PO\", \"Pay\"],\n",
    "            [\"Create SC\", \"Approve SC\", \"Create PO\", \"Pay\", \"Approve PO\"],            \n",
    "            [\"Create PO\", \"Approve PO\", \"Create SC\", \"Approve SC\", \"Pay\"]\n",
    "        ]\n",
    "        self.event_user_mapping = {\n",
    "            \"Create SC\": [\"Dev\", \"Chantal\"],\n",
    "            \"Approve SC\": [\"Kaly\"],\n",
    "            \"Create PO\": [\"Dev\", \"Jonas\"],\n",
    "            \"Approve PO\": [\"Kaly\"],\n",
    "            \"Pay\": [\"Seokju\"]\n",
    "        }\n",
    "        self.event_user_mapping_inv = {\n",
    "            \"Dev\": [\"Create SC\", \"Create PO\"],\n",
    "            \"Chantal\": [\"Create SC\"],\n",
    "            \"Kaly\": [\"Approve SC\", \"Approve PO\"],\n",
    "            \"Jonas\": [\"Create PO\"],\n",
    "            \"Seokju\": [\"Pay\"]\n",
    "        }\n",
    "        self.anomaly_probability = 0.3\n",
    "        self.max_cases = 1000\n",
    "        self.actual_cases = 0\n",
    "        if max_cases is not None:\n",
    "            self.max_cases = max_cases\n",
    "        if anomaly_probabilty is not None:\n",
    "            self.anomaly_probability = anomaly_probabilty\n",
    "        self.raw_dataset = []\n",
    "        self.case_id_counter = 0\n",
    "        self.encoders = {}\n",
    "        self.encoders[\"name\"] = LabelEncoder()\n",
    "        self.encoders[\"name\"].fit(list(self.event_names))\n",
    "        print(f\"encoders[name]:\\n{[(str(c), str(t)) for c, t in list(zip(self.encoders['name'].classes_, self.encoders['name'].transform(self.encoders['name'].classes_)))]}\")\n",
    "        self.encoders[\"user\"] = LabelEncoder()\n",
    "        self.encoders[\"user\"].fit(list(self.user_names))\n",
    "        print(f\"encoders[user]:\\n{[(str(c), str(t)) for c, t in list(zip(self.encoders['user'].classes_, self.encoders['user'].transform(self.encoders['user'].classes_)))]}\")\n",
    "        \n",
    "        \n",
    "    def create_valid_traces(self, num_traces):\n",
    "        partial_dataset = []\n",
    "        cases_per_trace = num_traces // len(self.valid_traces)\n",
    "        for valid_trace in self.valid_traces:\n",
    "            for _ in range(cases_per_trace):\n",
    "                case = Case()\n",
    "                event = Event()\n",
    "                for event_name in valid_trace:\n",
    "                    event[\"name\"] = event_name\n",
    "                    event[\"user\"] = np.random.choice(self.event_user_mapping[event_name])\n",
    "                    event[\"case_id\"] = self.case_id_counter\n",
    "                    # case.append(event.copy())\n",
    "                    partial_dataset.append(event.copy())\n",
    "                self.case_id_counter += 1\n",
    "                # partial_dataset.append(case)\n",
    "        return partial_dataset, len(partial_dataset)//5\n",
    "                \n",
    "    def create_invalid_traces_control_flow(self, num_traces):\n",
    "        partial_dataset = []\n",
    "        cases_per_trace = num_traces // len(self.invalid_traces_control_flow)\n",
    "        for invalid_trace in self.invalid_traces_control_flow:\n",
    "            for _ in range(cases_per_trace):\n",
    "                case = Case()\n",
    "                event = Event()\n",
    "                for event_name in invalid_trace:\n",
    "                    event[\"name\"] = event_name\n",
    "                    event[\"user\"] = np.random.choice(self.event_user_mapping[event_name])\n",
    "                    event[\"case_id\"] = self.case_id_counter\n",
    "                    # case.append(event.copy())\n",
    "                    partial_dataset.append(event.copy())\n",
    "                self.case_id_counter += 1\n",
    "                # partial_dataset.append(case)\n",
    "        return partial_dataset, len(partial_dataset)//5\n",
    "    \n",
    "    def create_invalid_traces_attribute(self, num_traces):\n",
    "        partial_dataset = []\n",
    "        cases_per_trace = num_traces // len(self.valid_traces)\n",
    "        for valid_trace in self.valid_traces:\n",
    "            for _ in range(cases_per_trace):\n",
    "                case = Case()\n",
    "                event = Event()\n",
    "                for event_name in valid_trace:\n",
    "                    event[\"name\"] = event_name\n",
    "                    wrong_users = set(self.user_names) - set(self.event_user_mapping[event_name])\n",
    "                    event[\"user\"] = np.random.choice(list(wrong_users))\n",
    "                    event[\"case_id\"] = self.case_id_counter\n",
    "                    # case.append(event.copy())\n",
    "                    partial_dataset.append(event.copy())\n",
    "                self.case_id_counter += 1\n",
    "                # partial_dataset.append(case)\n",
    "        return partial_dataset, len(partial_dataset)//5\n",
    "    \n",
    "    def create_dataset(self, max_cases=None, anomaly_probabilty=None):\n",
    "        if max_cases is None:\n",
    "            max_cases = self.max_cases\n",
    "        else:\n",
    "            self.max_cases = max_cases\n",
    "        if anomaly_probabilty is None:\n",
    "            anomaly_probabilty = self.anomaly_probability\n",
    "        else:\n",
    "            self.anomaly_probability = anomaly_probabilty\n",
    "        num_anomalous_cases = int(max_cases * anomaly_probabilty)\n",
    "        num_normal_cases = max_cases - num_anomalous_cases\n",
    "        self.actual_cases = []\n",
    "        raw_dataset, actual_count = self.create_valid_traces(num_normal_cases)\n",
    "        self.raw_dataset += raw_dataset\n",
    "        self.actual_cases += [actual_count]\n",
    "        raw_dataset, actual_count = self.create_invalid_traces_control_flow(num_anomalous_cases // 2)\n",
    "        self.raw_dataset += raw_dataset\n",
    "        self.actual_cases += [actual_count]\n",
    "        raw_dataset, actual_count = self.create_invalid_traces_attribute(num_anomalous_cases // 2)\n",
    "        self.raw_dataset += raw_dataset\n",
    "        self.actual_cases += [actual_count]\n",
    "    \n",
    "    @property\n",
    "    def raw_dataset_as_df(self):\n",
    "        # name, user, case_id\"\n",
    "        return pd.DataFrame(self.raw_dataset)\n",
    "    \n",
    "    @property\n",
    "    def raw_dataset_as_np_array(self):\n",
    "        np_df = self.raw_dataset_as_df.to_numpy().reshape(-1, 5, 3)\n",
    "        return np_df\n",
    "    \n",
    "    @property\n",
    "    def encoded_features(self):\n",
    "        without_case_id = self.raw_dataset_as_np_array[:, :, 0:2] # name, user\n",
    "        # without_case_id_one_row_one_case = without_case_id.reshape(-1, 10)\n",
    "        without_case_id_one_row_one_event = without_case_id.reshape(-1, 2)\n",
    "        just_names = without_case_id_one_row_one_event[:, 0]\n",
    "        just_users = without_case_id_one_row_one_event[:, 1]\n",
    "        encoded_names = self.encoders[\"name\"].transform(just_names).reshape(-1, 1)\n",
    "        encoded_users = self.encoders[\"user\"].transform(just_users).reshape(-1, 1)\n",
    "        encoded_data = np.hstack((encoded_names, encoded_users)).reshape(-1, 10)\n",
    "        return encoded_data\n",
    "    \n",
    "    @property\n",
    "    def one_hot_encoded_features(self):\n",
    "        one_row_one_event = self.encoded_features.reshape(-1, 2)\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        name_column = one_row_one_event[:, 0].reshape(-1, 1)  # Get name column\n",
    "        user_column = one_row_one_event[:, 1].reshape(-1, 1)  # Get user column\n",
    "\n",
    "        self.one_hot_encoder = {}\n",
    "        self.one_hot_encoder[\"name\"] = OneHotEncoder(sparse_output=False)\n",
    "        self.one_hot_encoder[\"user\"]= OneHotEncoder(sparse_output=False)\n",
    "\n",
    "        one_hot_encoded_name = self.one_hot_encoder[\"name\"].fit_transform(name_column).reshape(-1, 5)\n",
    "        one_hot_encoded_user = self.one_hot_encoder[\"user\"].fit_transform(user_column).reshape(-1, 5)\n",
    "        \n",
    "        one_hot_encoded_data = np.hstack((one_hot_encoded_name, one_hot_encoded_user)).reshape(-1, 50)\n",
    "        return one_hot_encoded_data\n",
    "    \n",
    "    @property\n",
    "    def one_hot_encoded_features_2d(self):\n",
    "        return self.one_hot_encoded_features.reshape(-1, 10, 5)\n",
    "    \n",
    "    @property\n",
    "    def x_one_hot_y_int(self):\n",
    "        return self.one_hot_encoded_features, self.encoded_features.reshape\n",
    "    \n",
    "    @property\n",
    "    def x_one_hot_2d_y_int_2d(self):\n",
    "        return self.one_hot_encoded_features_2d, self.encoded_features.reshape(-1, 10, 1)\n",
    "    \n",
    "    @property\n",
    "    def x_one_hot_2d_y_int(self):\n",
    "        #reshape is not needed here because it's already (-1, 10)\n",
    "        return self.one_hot_encoded_features_2d, self.encoded_features.reshape(-1, 10)\n",
    "    \n",
    "    def inverse_one_hot_encoded_features_to_int(self, one_hot_encoded_data):\n",
    "        \"\"\"\n",
    "        There are 50 columns in a completely one hot encoded data\n",
    "        thats 5 events and 5 users altenately\n",
    "        first we deconde to integer level\n",
    "        this function is just for one vector but maybe can be broadcasted over the whole dataset\n",
    "        \"\"\"\n",
    "        one_row_one_event = one_hot_encoded_data.reshape(-1, 10)\n",
    "        name_column = one_row_one_event[:, 0:5].reshape(-1, 5)\n",
    "        user_column = one_row_one_event[:, 5:10].reshape(-1, 5)\n",
    "        de_encoded_name = self.one_hot_encoder[\"name\"].inverse_transform(name_column).reshape(-1, 1)\n",
    "        de_encoded_user = self.one_hot_encoder[\"user\"].inverse_transform(user_column).reshape(-1, 1)\n",
    "        de_encoded_data = np.hstack((de_encoded_name, de_encoded_user)).reshape(-1, 2)\n",
    "        return de_encoded_data.reshape(-1, 10)\n",
    "\n",
    "    def inverse_one_hot_encoded_features_to_string(self, one_hot_encoded_data):\n",
    "        \"\"\"\n",
    "        There are 50 columns in a completely one hot encoded data\n",
    "        thats 5 events and 5 users altenately\n",
    "        first we deconde to integer level\n",
    "        then to string level\n",
    "        this function is just for one vector but maybe can be broadcasted over the whole dataset\n",
    "        \"\"\"\n",
    "        one_hot_encoded_data_int = self.inverse_one_hot_encoded_features_to_int(one_hot_encoded_data)\n",
    "        one_row_one_event = one_hot_encoded_data_int.reshape(-1, 2)\n",
    "        name_column = one_row_one_event[:, 0]\n",
    "        user_column = one_row_one_event[:, 1]\n",
    "        de_encoded_name = self.encoders[\"name\"].inverse_transform(name_column).reshape(-1, 1)\n",
    "        de_encoded_user = self.encoders[\"user\"].inverse_transform(user_column).reshape(-1, 1)\n",
    "        de_encoded_data = np.hstack((de_encoded_name, de_encoded_user)).reshape(-1, 2)\n",
    "        return de_encoded_data.reshape(-1, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "    pass\n",
    "    \n",
    "synth_dataset = Dataset(max_cases=1000, anomaly_probabilty=0.3)\n",
    "synth_dataset.create_dataset()\n",
    "print(f\"Cases Distribution: {synth_dataset.actual_cases}\")\n",
    "one_hot_flat = synth_dataset.one_hot_encoded_features\n",
    "print(f\"one_hot_flat shape: {one_hot_flat.shape}\")\n",
    "print(f\"Raw features test:\\n{synth_dataset.raw_dataset_as_df[:5]}\")\n",
    "print(f\"Encoded Features test:\\n{synth_dataset.encoded_features[0]}\")\n",
    "# pprint(f\"One hot Encoded feature test: {one_hot_flat[0]}\")\n",
    "print(f\"De encoding test:\\n{synth_dataset.inverse_one_hot_encoded_features_to_int(one_hot_flat[0])}\")\n",
    "# from pprint import pprint\n",
    "# pprint(f\"Encoded Features: {temp.shape}\")\n",
    "# temp = synth_dataset.one_hot_encoded_features\n",
    "# pprint(f\"One hot Encoded Features: {temp.shape}\")\n",
    "# int_de_encode = synth_dataset.inverse_one_hot_encoded_features_to_int(temp[0])\n",
    "# print(f\"De encoding test: {int_de_encode}\")\n",
    "# str_de_encode = synth_dataset.inverse_one_hot_encoded_features_to_string(temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(dataset: Dataset):\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout, GaussianNoise\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    hidden_layers = 2\n",
    "    hidden_size_factor = .2\n",
    "    noise = True\n",
    "\n",
    "    features = dataset.one_hot_encoded_features\n",
    "\n",
    "    # Parameters\n",
    "    input_size = features.shape[1]\n",
    "\n",
    "    # Input layer\n",
    "    input = Input(shape=(input_size,), name='input')\n",
    "    x = input\n",
    "\n",
    "    # Noise layer\n",
    "    if noise is not None:\n",
    "        x = GaussianNoise(noise)(x)\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        if isinstance(hidden_size_factor, list):\n",
    "            factor = hidden_size_factor[i]\n",
    "        else:\n",
    "            factor = hidden_size_factor\n",
    "        x = Dense(int(input_size * factor), activation='relu', name=f'hid{i + 1}')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(input_size, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n",
    "        loss='mean_squared_error',\n",
    "    )\n",
    "\n",
    "    return model, features, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model, _, _ = model_fn(synth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 50)]              0         \n",
      "                                                                 \n",
      " gaussian_noise_1 (GaussianN  (None, 50)               0         \n",
      " oise)                                                           \n",
      "                                                                 \n",
      " hid1 (Dense)                (None, 10)                510       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " hid2 (Dense)                (None, 10)                110       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 50)                550       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2048 - val_loss: 0.2240\n",
      "Epoch 2/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2035 - val_loss: 0.2235\n",
      "Epoch 3/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2030 - val_loss: 0.2230\n",
      "Epoch 4/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2041 - val_loss: 0.2225\n",
      "Epoch 5/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2026 - val_loss: 0.2220\n",
      "Epoch 6/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2028 - val_loss: 0.2216\n",
      "Epoch 7/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1982 - val_loss: 0.2211\n",
      "Epoch 8/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1976 - val_loss: 0.2206\n",
      "Epoch 9/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1995 - val_loss: 0.2201\n",
      "Epoch 10/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1975 - val_loss: 0.2195\n",
      "Epoch 11/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1944 - val_loss: 0.2190\n",
      "Epoch 12/70\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1938 - val_loss: 0.2185\n",
      "Epoch 13/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1945 - val_loss: 0.2180\n",
      "Epoch 14/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1932 - val_loss: 0.2174\n",
      "Epoch 15/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1946 - val_loss: 0.2169\n",
      "Epoch 16/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1929 - val_loss: 0.2164\n",
      "Epoch 17/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1909 - val_loss: 0.2158\n",
      "Epoch 18/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1890 - val_loss: 0.2153\n",
      "Epoch 19/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1890 - val_loss: 0.2147\n",
      "Epoch 20/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1864 - val_loss: 0.2142\n",
      "Epoch 21/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1853 - val_loss: 0.2136\n",
      "Epoch 22/70\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1852 - val_loss: 0.2130\n",
      "Epoch 23/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1853 - val_loss: 0.2124\n",
      "Epoch 24/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1810 - val_loss: 0.2118\n",
      "Epoch 25/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1829 - val_loss: 0.2112\n",
      "Epoch 26/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1809 - val_loss: 0.2106\n",
      "Epoch 27/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1819 - val_loss: 0.2101\n",
      "Epoch 28/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1795 - val_loss: 0.2095\n",
      "Epoch 29/70\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1786 - val_loss: 0.2089\n",
      "Epoch 30/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1787 - val_loss: 0.2083\n",
      "Epoch 31/70\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1796 - val_loss: 0.2077\n",
      "Epoch 32/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1780 - val_loss: 0.2071\n",
      "Epoch 33/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1734 - val_loss: 0.2065\n",
      "Epoch 34/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1731 - val_loss: 0.2059\n",
      "Epoch 35/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1751 - val_loss: 0.2054\n",
      "Epoch 36/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1701 - val_loss: 0.2048\n",
      "Epoch 37/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1707 - val_loss: 0.2042\n",
      "Epoch 38/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1715 - val_loss: 0.2036\n",
      "Epoch 39/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1692 - val_loss: 0.2030\n",
      "Epoch 40/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1690 - val_loss: 0.2023\n",
      "Epoch 41/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1688 - val_loss: 0.2017\n",
      "Epoch 42/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1662 - val_loss: 0.2011\n",
      "Epoch 43/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1655 - val_loss: 0.2004\n",
      "Epoch 44/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1656 - val_loss: 0.1998\n",
      "Epoch 45/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1633 - val_loss: 0.1992\n",
      "Epoch 46/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1598 - val_loss: 0.1986\n",
      "Epoch 47/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1616 - val_loss: 0.1979\n",
      "Epoch 48/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1604 - val_loss: 0.1973\n",
      "Epoch 49/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1604 - val_loss: 0.1967\n",
      "Epoch 50/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1582 - val_loss: 0.1960\n",
      "Epoch 51/70\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1577 - val_loss: 0.1954\n",
      "Epoch 52/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1577 - val_loss: 0.1948\n",
      "Epoch 53/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1583 - val_loss: 0.1942\n",
      "Epoch 54/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1553 - val_loss: 0.1935\n",
      "Epoch 55/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1550 - val_loss: 0.1929\n",
      "Epoch 56/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1490 - val_loss: 0.1923\n",
      "Epoch 57/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1532 - val_loss: 0.1917\n",
      "Epoch 58/70\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1515 - val_loss: 0.1910\n",
      "Epoch 59/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1490 - val_loss: 0.1904\n",
      "Epoch 60/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1493 - val_loss: 0.1898\n",
      "Epoch 61/70\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1499 - val_loss: 0.1892\n",
      "Epoch 62/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1455 - val_loss: 0.1885\n",
      "Epoch 63/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1465 - val_loss: 0.1879\n",
      "Epoch 64/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1468 - val_loss: 0.1873\n",
      "Epoch 65/70\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1457 - val_loss: 0.1867\n",
      "Epoch 66/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1405 - val_loss: 0.1861\n",
      "Epoch 67/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1445 - val_loss: 0.1855\n",
      "Epoch 68/70\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1413 - val_loss: 0.1849\n",
      "Epoch 69/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1401 - val_loss: 0.1843\n",
      "Epoch 70/70\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1387 - val_loss: 0.1837\n"
     ]
    }
   ],
   "source": [
    "history = dae_model.fit(\n",
    "    one_hot_flat, one_hot_flat,\n",
    "    epochs=70,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1.]])\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "array([[0.2739193 , 0.24642536, 0.2337602 , 0.85556823, 0.36841056,\n",
      "        0.5256383 , 0.50858957, 0.2868952 , 0.14678694, 0.4635234 ,\n",
      "        0.22361268, 0.69030535, 0.27700204, 0.16933021, 0.29169515,\n",
      "        0.28178638, 0.31617442, 0.23962119, 0.77750623, 0.20826949,\n",
      "        0.42237478, 0.20493293, 0.52529866, 0.15338881, 0.1778515 ,\n",
      "        0.29552883, 0.46022958, 0.45655587, 0.2667199 , 0.3328998 ,\n",
      "        0.7781956 , 0.28047216, 0.41250315, 0.3798826 , 0.30386594,\n",
      "        0.52685326, 0.43211424, 0.24897757, 0.70690864, 0.21838123,\n",
      "        0.24589202, 0.18628019, 0.26893368, 0.13125217, 0.6247224 ,\n",
      "        0.24578662, 0.18616524, 0.41130507, 0.27672476, 0.70805293]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_vector = one_hot_flat[0].reshape(1, -1)\n",
    "pprint(test_vector)\n",
    "op = dae_model.predict(test_vector)\n",
    "pprint(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(op.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2d_fn(dataset: Dataset):\n",
    "    \"\"\"\n",
    "    Two dimentional variational autoencoder\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout, GaussianNoise, Reshape, Flatten, Concatenate\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    hidden_layers = 2\n",
    "    hidden_size_factor = .2\n",
    "    noise = True\n",
    "\n",
    "    features = dataset.one_hot_encoded_features_2d\n",
    "\n",
    "    # Parameters\n",
    "    input_size_1 = features.shape[1]\n",
    "    input_size_2 = features.shape[2]\n",
    "    \n",
    "\n",
    "    # Input layer\n",
    "    input = Input(shape=(input_size_1, input_size_2, ), name='input')\n",
    "    flat_input = Flatten()(input)\n",
    "    x = flat_input\n",
    "\n",
    "    # Noise layer\n",
    "    if noise is not None:\n",
    "        x = GaussianNoise(noise)(x)\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(hidden_layers):\n",
    "        if isinstance(hidden_size_factor, list):\n",
    "            factor = hidden_size_factor[i]\n",
    "        else:\n",
    "            factor = hidden_size_factor\n",
    "        x = Dense(int(input_size_1 * input_size_2 * factor), activation='relu', name=f'hid{i + 1}')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = []\n",
    "    for i in range(input_size_1):\n",
    "        output = Dense(input_size_2, activation='sigmoid', name=f'output_{i}')(x)\n",
    "        outputs.append(output)\n",
    "\n",
    "    outputs = Concatenate(axis=1)(outputs)\n",
    "    outputs = Reshape((input_size_1, input_size_2))(outputs)\n",
    "    # Build model\n",
    "    model = Model(inputs=input, outputs=outputs)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001, beta_2=0.99),\n",
    "        loss='mean_squared_error',\n",
    "    )\n",
    "\n",
    "    return model, features, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 10, 5)]      0           []                               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 50)           0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " gaussian_noise_4 (GaussianNois  (None, 50)          0           ['flatten_2[0][0]']              \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " hid1 (Dense)                   (None, 10)           510         ['gaussian_noise_4[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 10)           0           ['hid1[0][0]']                   \n",
      "                                                                                                  \n",
      " hid2 (Dense)                   (None, 10)           110         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 10)           0           ['hid2[0][0]']                   \n",
      "                                                                                                  \n",
      " output_0 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_1 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_2 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_3 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_4 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_5 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_6 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_7 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_8 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " output_9 (Dense)               (None, 5)            55          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 50)           0           ['output_0[0][0]',               \n",
      "                                                                  'output_1[0][0]',               \n",
      "                                                                  'output_2[0][0]',               \n",
      "                                                                  'output_3[0][0]',               \n",
      "                                                                  'output_4[0][0]',               \n",
      "                                                                  'output_5[0][0]',               \n",
      "                                                                  'output_6[0][0]',               \n",
      "                                                                  'output_7[0][0]',               \n",
      "                                                                  'output_8[0][0]',               \n",
      "                                                                  'output_9[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 10, 5)        0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dae_model_2d, _, _ = model_2d_fn(synth_dataset)\n",
    "dae_model_2d.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "8/8 [==============================] - 1s 29ms/step - loss: 0.3098 - val_loss: 0.2730\n",
      "Epoch 2/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3155 - val_loss: 0.2723\n",
      "Epoch 3/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3072 - val_loss: 0.2717\n",
      "Epoch 4/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3089 - val_loss: 0.2711\n",
      "Epoch 5/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3121 - val_loss: 0.2705\n",
      "Epoch 6/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3027 - val_loss: 0.2699\n",
      "Epoch 7/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3050 - val_loss: 0.2693\n",
      "Epoch 8/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3024 - val_loss: 0.2688\n",
      "Epoch 9/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3032 - val_loss: 0.2682\n",
      "Epoch 10/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2981 - val_loss: 0.2676\n",
      "Epoch 11/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3023 - val_loss: 0.2671\n",
      "Epoch 12/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3009 - val_loss: 0.2665\n",
      "Epoch 13/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2980 - val_loss: 0.2660\n",
      "Epoch 14/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2969 - val_loss: 0.2655\n",
      "Epoch 15/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2953 - val_loss: 0.2650\n",
      "Epoch 16/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2951 - val_loss: 0.2645\n",
      "Epoch 17/60\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2931 - val_loss: 0.2640\n",
      "Epoch 18/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2938 - val_loss: 0.2635\n",
      "Epoch 19/60\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2913 - val_loss: 0.2631\n",
      "Epoch 20/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2935 - val_loss: 0.2626\n",
      "Epoch 21/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2883 - val_loss: 0.2621\n",
      "Epoch 22/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2916 - val_loss: 0.2617\n",
      "Epoch 23/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2886 - val_loss: 0.2613\n",
      "Epoch 24/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2849 - val_loss: 0.2608\n",
      "Epoch 25/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2856 - val_loss: 0.2603\n",
      "Epoch 26/60\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2839 - val_loss: 0.2599\n",
      "Epoch 27/60\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2845 - val_loss: 0.2595\n",
      "Epoch 28/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2852 - val_loss: 0.2590\n",
      "Epoch 29/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2840 - val_loss: 0.2586\n",
      "Epoch 30/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2796 - val_loss: 0.2582\n",
      "Epoch 31/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2797 - val_loss: 0.2578\n",
      "Epoch 32/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2814 - val_loss: 0.2574\n",
      "Epoch 33/60\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2786 - val_loss: 0.2570\n",
      "Epoch 34/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2819 - val_loss: 0.2566\n",
      "Epoch 35/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2802 - val_loss: 0.2562\n",
      "Epoch 36/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2724 - val_loss: 0.2558\n",
      "Epoch 37/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2751 - val_loss: 0.2554\n",
      "Epoch 38/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2754 - val_loss: 0.2550\n",
      "Epoch 39/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2758 - val_loss: 0.2547\n",
      "Epoch 40/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2738 - val_loss: 0.2543\n",
      "Epoch 41/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2706 - val_loss: 0.2539\n",
      "Epoch 42/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2712 - val_loss: 0.2536\n",
      "Epoch 43/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2675 - val_loss: 0.2532\n",
      "Epoch 44/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2712 - val_loss: 0.2529\n",
      "Epoch 45/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2692 - val_loss: 0.2526\n",
      "Epoch 46/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2700 - val_loss: 0.2522\n",
      "Epoch 47/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2665 - val_loss: 0.2519\n",
      "Epoch 48/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2670 - val_loss: 0.2516\n",
      "Epoch 49/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2657 - val_loss: 0.2513\n",
      "Epoch 50/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2679 - val_loss: 0.2510\n",
      "Epoch 51/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2647 - val_loss: 0.2507\n",
      "Epoch 52/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2663 - val_loss: 0.2503\n",
      "Epoch 53/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2622 - val_loss: 0.2500\n",
      "Epoch 54/60\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2640 - val_loss: 0.2497\n",
      "Epoch 55/60\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2612 - val_loss: 0.2494\n",
      "Epoch 56/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2622 - val_loss: 0.2491\n",
      "Epoch 57/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2624 - val_loss: 0.2488\n",
      "Epoch 58/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2593 - val_loss: 0.2485\n",
      "Epoch 59/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2579 - val_loss: 0.2482\n",
      "Epoch 60/60\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2573 - val_loss: 0.2479\n"
     ]
    }
   ],
   "source": [
    "history_2d = dae_model_2d.fit(\n",
    "    synth_dataset.one_hot_encoded_features_2d, synth_dataset.one_hot_encoded_features_2d,\n",
    "    epochs=60,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1.]]])\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "array([[[0.48761427, 0.5015451 , 0.4773597 , 0.52389514, 0.49006116],\n",
      "        [0.5001155 , 0.5040638 , 0.48521015, 0.50037163, 0.46817517],\n",
      "        [0.50422937, 0.5264716 , 0.46972108, 0.49066883, 0.5076579 ],\n",
      "        [0.4751231 , 0.47617364, 0.49726123, 0.5187941 , 0.49022898],\n",
      "        [0.5045805 , 0.4825806 , 0.5179023 , 0.4880969 , 0.47701532],\n",
      "        [0.48949397, 0.4944772 , 0.5036503 , 0.4638604 , 0.49131078],\n",
      "        [0.5261544 , 0.47858143, 0.48469475, 0.48980522, 0.48459852],\n",
      "        [0.48354363, 0.46886715, 0.4872648 , 0.5188754 , 0.4859896 ],\n",
      "        [0.47369137, 0.48774937, 0.48405817, 0.46121997, 0.4997255 ],\n",
      "        [0.47668332, 0.48584104, 0.5075947 , 0.4818354 , 0.52130926]]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_vector_2d = synth_dataset.one_hot_encoded_features_2d[0].reshape(-1, 10, 5)\n",
    "pprint(test_vector_2d)\n",
    "op = dae_model_2d.predict(test_vector_2d)\n",
    "pprint(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
